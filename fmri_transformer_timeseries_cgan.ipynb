{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBPnIdWnzbD1"
   },
   "source": [
    "# fMRI Transformer Conditional GAN\n",
    "\n",
    "A conditional Transformer based time-series GAN that generates images from 1-D signal data over 360 channels, conditioned over 3 classes.\n",
    "\n",
    "*Author's note: this is a reduced version of the full model intended for accessible use by others*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABBavdI3zOkx",
    "outputId": "596396f6-de4c-46c9-cccd-bb8ab1c66d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.4.1\n"
     ]
    }
   ],
   "source": [
    "#@title Imports and Connect Drive (Optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "!pip install einops\n",
    "\n",
    "import os, sys\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import dill\n",
    "path = Path(\"SAMPLE DIRECTORY PATH\")\n",
    "os.chdir(path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch import optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import Tensor\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "KNbINzNy0F9O"
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "ZLemprmKzzNA"
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Handles variability by controlling sources of randomness\n",
    "  through set seed values\n",
    "\n",
    "  Args:\n",
    "    seed: Integer\n",
    "      Set the seed value to given integer.\n",
    "      If no seed, set seed value to random integer in the range 2^32\n",
    "    seed_torch: Bool\n",
    "      Seeds the random number generator for all devices to\n",
    "      offer some guarantees on reproducibility\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDTyGmigz9rP",
    "outputId": "c39b006e-3824-4046-9cde-aaa7553050f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n",
      "GPU is enabled in this notebook.\n"
     ]
    }
   ],
   "source": [
    "# Set global variables\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLqbdpFj_JiK"
   },
   "source": [
    "# DATASET NOTE:\n",
    "Due to the architecture of the TTS-GAN, we do not reduce the data to its mean, and instead opt for batch sizes of 33 to match the sequence of events that occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "47mIoZ55-5p_"
   },
   "outputs": [],
   "source": [
    "#@title Create Dataset\n",
    "\n",
    "class FMRIDataset(Dataset):\n",
    "    \"\"\"HCP Gambling FMRI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_file=\"data_fmri_all_w_init.csv\", \n",
    "                 labels_file=\"data_events_all_w_init.csv\", \n",
    "                 root_dir=\"/content/drive/MyDrive/nmaproject/\", \n",
    "                 transform=None,onehot=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file (string): Path to the csv file with each subject's fMRI readings\n",
    "            label_file(string): Path to the csv file with the outcome of each trial\n",
    "            root_dir  (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        def f(x):\n",
    "          if x == \"init\" or x == \"neut\": return 0\n",
    "          elif x == \"loss\": return 1\n",
    "          else: return 2\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        data = pd.read_csv(self.root_dir + data_file).drop(columns=\"Unnamed: 0\")\n",
    "        data = torch.tensor(data.values)\n",
    "        # OG: self.data = data.reshape(100,360,33,5).mean(dim=3).unsqueeze(2).type(torch.FloatTensor)\n",
    "        self.data = data.reshape(100,360,33,5).reshape((33*100,360,1,5)).type(torch.FloatTensor)\n",
    "\n",
    "        labels = pd.read_csv(self.root_dir + labels_file).drop(columns=\"Unnamed: 0\")\n",
    "        labels_mapped = labels.applymap(f)\n",
    "        labels = torch.tensor(labels_mapped.values)\n",
    "        # OG: self.labels = labels.reshape(100,33)\n",
    "        self.labels = labels.reshape((33 * 100,))\n",
    "        if onehot:\n",
    "          self.labels = F.one_hot(self.labels) # OG: (100,33,3), NEW: (3300,3)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        x = self.data[idx] # OG: (360,1,33), NEW: (360,1,5)\n",
    "        y = self.labels[idx] # OG: (33,3), NEW: (5) OR (5,3) if one-hot\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        sample = (x,y)\n",
    "\n",
    "        return sample\n",
    "\n",
    "def custom_normalize(x):\n",
    "    \"\"\" A custom normalization method\n",
    "        Returns\n",
    "            result: a normalized epoch\n",
    "    \"\"\"\n",
    "    e = 1e-10\n",
    "    result = (x - x.mean(axis=0)) / ((torch.sqrt(x.var(axis=0)))+e)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Lk0S69FNiWA"
   },
   "source": [
    "### Defining Parameters and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cCV5DUO10L02"
   },
   "outputs": [],
   "source": [
    "# training_parameters\n",
    "cond = True # conditional GAN vs regular GAN\n",
    "shrink = 10\n",
    "pars = {\n",
    "    \"n_epochs\": 11,#1000,\n",
    "    \"batch_size\": 33,\n",
    "    \"g_lr\": 0.0001,\n",
    "    \"d_lr\": 0.0003, # discriminator learning rate\n",
    "    \"wd\": 1e-3,     # weight decay\n",
    "    \"beta1\": 0.9,  # for adam\n",
    "    \"beta2\": 0.999, # for adam\n",
    "    \"n_critic\": 1,\n",
    "    \"latent\":(3*360*33 // 2) // shrink, # due to RAM constraints\n",
    "    \"embed_dim\":(3*36*33 // shrink) + (3*36*33 % 5),\n",
    "    \"dis_embed_dim\":(3*36*33 // shrink) + (3*36*33 % 5),#(330) // shrink,\n",
    "    \"ema_kimg\":500,\n",
    "    \"ema_warmup\":0.1,\n",
    "    \"ema\":0.9999,\n",
    "    \"global_steps\":0,\n",
    "    \"patch_size\":1, #3\n",
    "    \"seq_len\":5,\n",
    "}\n",
    "\n",
    "dataset = FMRIDataset(transform=custom_normalize,onehot=False) # onehot should be False for CGAN\n",
    "data_loader = DataLoader(dataset, batch_size=pars[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "srgwqW5NHXEi"
   },
   "outputs": [],
   "source": [
    "#@title Define Models\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, seq_len=pars[\"seq_len\"], channels=360, num_classes=3, latent_dim=pars[\"latent\"], embed_dim=pars[\"embed_dim\"], depth=3,\n",
    "                 num_heads=5, forward_drop_rate=0.5, attn_drop_rate=0.5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depth = depth\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.forward_drop_rate = forward_drop_rate\n",
    "        \n",
    "        self.l1 = nn.Linear(self.latent_dim, self.seq_len * self.embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.seq_len, self.embed_dim))\n",
    "        self.blocks = Gen_TransformerEncoder(\n",
    "                         depth=self.depth,\n",
    "                         emb_size = self.embed_dim,\n",
    "                         drop_p = self.attn_drop_rate,\n",
    "                         forward_drop_p=self.forward_drop_rate\n",
    "                        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.channels, 1, 1, 0)\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.l1(z).view(-1, self.seq_len, self.embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        H, W = 1, self.seq_len\n",
    "        x = self.blocks(x)\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "        output = self.deconv(x.permute(0, 3, 1, 2))\n",
    "        output = output.view(-1, self.channels, H, W)\n",
    "        return output\n",
    "\n",
    "# Conditional GAN\n",
    "class CGenerator(nn.Module):\n",
    "    def __init__(self, seq_len=pars[\"seq_len\"], channels=360, num_classes=3, latent_dim=pars[\"latent\"], data_embed_dim=pars[\"embed_dim\"], \n",
    "                label_embed_dim=pars[\"embed_dim\"] ,depth=3, num_heads=5, \n",
    "                forward_drop_rate=0.5, attn_drop_rate=0.5):\n",
    "\n",
    "        super(CGenerator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.channels = channels\n",
    "        self.num_classes = num_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_embed_dim = data_embed_dim\n",
    "        self.label_embed_dim = label_embed_dim\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.forward_drop_rate = forward_drop_rate\n",
    "        \n",
    "        self.l1 = nn.Linear(self.latent_dim + self.label_embed_dim, self.seq_len * self.data_embed_dim)\n",
    "        self.label_embedding = nn.Embedding(self.num_classes, self.label_embed_dim) \n",
    "        \n",
    "        self.blocks = Gen_TransformerEncoder(\n",
    "                 depth=self.depth,\n",
    "                 emb_size = self.data_embed_dim,\n",
    "                 num_heads = self.num_heads,\n",
    "                 drop_p = self.attn_drop_rate,\n",
    "                 forward_drop_p=self.forward_drop_rate\n",
    "                )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.Conv2d(self.data_embed_dim, self.channels, 1, 1, 0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_embedding(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        x = self.l1(x)\n",
    "        x = x.view(-1, self.seq_len, self.data_embed_dim)\n",
    "        H, W = 1, self.seq_len\n",
    "        x = self.blocks(x)\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "        output = self.deconv(x.permute(0, 3, 1, 2))\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "class Gen_TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 num_heads=5,\n",
    "                 drop_p=0.5,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.5):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "        \n",
    "class Gen_TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=8, **kwargs):\n",
    "        super().__init__(*[Gen_TransformerEncoderBlock(**kwargs) for _ in range(depth)])       \n",
    "        \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "class Dis_TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size=100,\n",
    "                 num_heads=5,\n",
    "                 drop_p=0.,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class Dis_TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=8, **kwargs):\n",
    "        super().__init__(*[Dis_TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "        \n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=100, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.clshead(x)\n",
    "        return out\n",
    "\n",
    "class C_ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=100, adv_classes=2, cls_classes=10):\n",
    "        super().__init__()\n",
    "        self.adv_head = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, adv_classes)\n",
    "        )\n",
    "        self.cls_head = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, cls_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_adv = self.adv_head(x)\n",
    "        out_cls = self.cls_head(x)\n",
    "        return out_adv, out_cls\n",
    "    \n",
    "class PatchEmbedding_Linear(nn.Module):\n",
    "    #what are the proper parameters set here?\n",
    "    def __init__(self, in_channels = 21, patch_size = 16, emb_size = 100, seq_length = 1024):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        #change the conv2d parameters here\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)',s1 = 1, s2 = patch_size),\n",
    "            nn.Linear(patch_size*in_channels, emb_size)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((seq_length // patch_size) + 1, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        #prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # position\n",
    "        x += self.positions\n",
    "        return x        \n",
    "        \n",
    "class Discriminator(nn.Sequential): # for CGAN parent from nn.Module\n",
    "    def __init__(self, \n",
    "                 in_channels=360,\n",
    "                 patch_size=pars[\"patch_size\"],\n",
    "                 emb_size=pars[\"dis_embed_dim\"], \n",
    "                 seq_length = pars[\"seq_len\"],\n",
    "                 depth=3, \n",
    "                 n_classes=1, #before 3 \n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__(\n",
    "              PatchEmbedding_Linear(in_channels, patch_size, emb_size, seq_length),\n",
    "              Dis_TransformerEncoder(depth, emb_size=emb_size, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
    "              ClassificationHead(emb_size, n_classes),\n",
    "          )\n",
    "        \n",
    "        @property\n",
    "        def device(self):\n",
    "            return next(self.parameters()).device\n",
    "\n",
    "class CDiscriminator(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_channels=360,\n",
    "                 patch_size=pars[\"patch_size\"],\n",
    "                 data_emb_size=pars[\"dis_embed_dim\"],\n",
    "                 label_emb_size=pars[\"dis_embed_dim\"],\n",
    "                 seq_length = pars[\"seq_len\"],\n",
    "                 depth=3, \n",
    "                 n_classes=3, \n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding_Linear(in_channels, patch_size, data_emb_size, seq_length),\n",
    "            Dis_TransformerEncoder(depth, emb_size=data_emb_size, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
    "            C_ClassificationHead(data_emb_size, 1, n_classes)\n",
    "        )\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX_oSuh9a-we"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nykJ_gENK5t"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9T1azQ0-P6r5"
   },
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "39CaRJgLP1fp"
   },
   "outputs": [],
   "source": [
    "#@title Load Model\n",
    "if load_model:\n",
    "  if cond:\n",
    "    model_name = \"tts-cgan\"\n",
    "  else:\n",
    "    model_name = \"tts-gan\"\n",
    "  checkpoint = torch.load(path / f'{model_name}_checkpoint.pth',map_location=DEVICE,pickle_module=dill)\n",
    "else:\n",
    "  checkpoint = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "id": "wo96JqpFQyLj"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions\n",
    "\n",
    "class LinearLrDecay(object):\n",
    "    def __init__(self, optimizer, start_lr, end_lr, decay_start_step, decay_end_step):\n",
    "\n",
    "        assert start_lr > end_lr\n",
    "        self.optimizer = optimizer\n",
    "        self.delta = (start_lr - end_lr) / (decay_end_step - decay_start_step)\n",
    "        self.decay_start_step = decay_start_step\n",
    "        self.decay_end_step = decay_end_step\n",
    "        self.start_lr = start_lr\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "    def step(self, current_step):\n",
    "        if current_step <= self.decay_start_step:\n",
    "            lr = self.start_lr\n",
    "        elif current_step >= self.decay_end_step:\n",
    "            lr = self.end_lr\n",
    "        else:\n",
    "            lr = self.start_lr - self.delta * (current_step - self.decay_start_step)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        return lr\n",
    "        \n",
    "def copy_params(model, mode='cpu'):\n",
    "    if mode == 'gpu':\n",
    "        flatten = []\n",
    "        for p in model.parameters():\n",
    "            cpu_p = deepcopy(p).cpu()\n",
    "            flatten.append(cpu_p.data)\n",
    "    else:\n",
    "        flatten = deepcopy(list(p.data for p in model.parameters()))\n",
    "    return flatten\n",
    "\n",
    "def gradient_penalty(y, x):\n",
    "    \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
    "    weight = torch.ones(y.size()).to(DEVICE)\n",
    "    dydx = torch.autograd.grad(outputs=y,\n",
    "                               inputs=x,\n",
    "                               grad_outputs=weight,\n",
    "                               retain_graph=True,\n",
    "                               create_graph=True,\n",
    "                               only_inputs=True)[0]\n",
    "\n",
    "    # dydx = dydx.view(dydx.size(0), -1)\n",
    "    dydx = dydx.reshape(dydx.size(0), -1)\n",
    "    dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
    "    return torch.mean((dydx_l2norm-1)**2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "id": "jH-r9_hUNIRZ"
   },
   "outputs": [],
   "source": [
    "#@title Training Loop\n",
    "def train(data_loader, checkpoint = {}, cond=False):\n",
    "    if checkpoint:\n",
    "      generator = checkpoint[\"gen_net\"]\n",
    "      discriminator = checkpoint[\"dis_net\"]\n",
    "      gen_optimizer = checkpoint[\"gen_optimizer\"]\n",
    "      dis_optimizer = checkpoint[\"dis_optimizer\"]\n",
    "      gen_scheduler = checkpoint[\"gen_scheduler\"]\n",
    "      dis_scheduler = checkpoint[\"dis_scheduler\"]\n",
    "      start_epoch = int(checkpoint[\"epoch\"])\n",
    "      global_steps = start_epoch * len(data_loader)\n",
    "    else:\n",
    "      if cond:\n",
    "        generator = CGenerator().to(DEVICE)\n",
    "        discriminator = CDiscriminator().to(DEVICE)\n",
    "      else:\n",
    "        generator = Generator().to(DEVICE)\n",
    "        discriminator = Discriminator().to(DEVICE)\n",
    "      gen_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "                                          pars[\"g_lr\"], (pars[\"beta1\"], pars[\"beta2\"]))\n",
    "      dis_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "                                      pars[\"d_lr\"], (pars[\"beta1\"], pars[\"beta2\"]))\n",
    "      gen_scheduler = LinearLrDecay(gen_optimizer, pars[\"g_lr\"], 0.0, 0, pars[\"n_epochs\"] * pars[\"n_critic\"])\n",
    "      dis_scheduler = LinearLrDecay(dis_optimizer, pars[\"d_lr\"], 0.0, 0, pars[\"n_epochs\"] * pars[\"n_critic\"])\n",
    "      start_epoch = 0\n",
    "      global_steps = 0\n",
    "\n",
    "    if cond:\n",
    "      model_name=\"tts-cgan\"\n",
    "    else:\n",
    "      model_name=\"tts-gan\"\n",
    "\n",
    "    gen_avg_param = copy_params(generator)\n",
    "    n_epochs = pars[\"n_epochs\"]\n",
    "\n",
    "    ## FOR CGAN\n",
    "    # adv_criterion = nn.BCELoss()\n",
    "    lambda_cls = 1 # same as paper\n",
    "    lambda_gp = 10\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set to Train Mode\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch_idx in range(start_epoch,n_epochs):\n",
    "        for iter_idx, (imgs, labels) in enumerate(tqdm(data_loader)):\n",
    "          # Adversarial ground truths\n",
    "            real_imgs = imgs.type(torch.FloatTensor).to(DEVICE)\n",
    "            real_img_labels = labels.type(torch.LongTensor).to(DEVICE)\n",
    "            noise = torch.randn(pars[\"batch_size\"],pars[\"latent\"]).to(DEVICE) # same as 'z' in TTS-GAN\n",
    "            fake_labels = torch.randint(0, 3, (pars[\"batch_size\"],)).to(DEVICE) # the 3 represents the number of classes\n",
    "\n",
    "            #Train Discriminator\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            if cond:\n",
    "              r_out_adv, r_out_cls = discriminator(real_imgs)\n",
    "              fake_imgs = generator(noise, fake_labels)\n",
    "            else:\n",
    "              real_validity = discriminator(real_imgs)\n",
    "              fake_imgs = generator(noise).detach()\n",
    "\n",
    "            assert fake_imgs.size() == real_imgs.size(), f\"fake_imgs.size(): {fake_imgs.size()} real_imgs.size(): {real_imgs.size()}\"\n",
    "\n",
    "\n",
    "            if cond:\n",
    "              f_out_adv, f_out_cls = discriminator(fake_imgs)\n",
    "              # Compute loss for gradient penalty.\n",
    "              alpha = torch.rand(real_imgs.size(0), 1, 1, 1).to(DEVICE)  # bh, C, H, W\n",
    "              x_hat = (alpha * real_imgs.data + (1 - alpha) * fake_imgs.data).requires_grad_(True)\n",
    "              out_src, _ = discriminator(x_hat)\n",
    "              d_loss_gp = gradient_penalty(out_src, x_hat)\n",
    "              \n",
    "              d_real_loss = -torch.mean(r_out_adv)\n",
    "              d_fake_loss = torch.mean(f_out_adv)\n",
    "              d_adv_loss = d_real_loss + d_fake_loss \n",
    "              \n",
    "              d_cls_loss = cls_criterion(r_out_cls, real_img_labels)\n",
    "              \n",
    "              d_loss = d_adv_loss + lambda_cls * d_cls_loss + lambda_gp * d_loss_gp\n",
    "              d_loss.backward()\n",
    "              \n",
    "              torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 5.)\n",
    "              dis_optimizer.step()\n",
    "            else:\n",
    "              fake_validity = discriminator(fake_imgs)\n",
    "\n",
    "              real_label = torch.full((real_validity.shape[0],real_validity.shape[1]), 1., dtype=torch.float, device=DEVICE)\n",
    "              fake_label = torch.full((real_validity.shape[0],real_validity.shape[1]), 0., dtype=torch.float, device=DEVICE)\n",
    "              d_real_loss = nn.MSELoss()(real_validity, real_label)\n",
    "              d_fake_loss = nn.MSELoss()(fake_validity, fake_label)\n",
    "              d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "              # skip accumulated_times since default is 1\n",
    "              d_loss.backward()\n",
    "\n",
    "              torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 5.)\n",
    "              dis_optimizer.step()\n",
    "              dis_optimizer.zero_grad()\n",
    "\n",
    "            # Train Generator\n",
    "            generator.zero_grad()\n",
    "\n",
    "            if cond:\n",
    "              gen_imgs = generator(noise, fake_labels)\n",
    "              g_out_adv, g_out_cls = discriminator(gen_imgs)\n",
    "\n",
    "              g_adv_loss = -torch.mean(g_out_adv)\n",
    "              g_cls_loss = cls_criterion(g_out_cls, fake_labels)    \n",
    "              g_loss = g_adv_loss + lambda_cls * g_cls_loss\n",
    "              g_loss.backward()\n",
    "\n",
    "              torch.nn.utils.clip_grad_norm_(generator.parameters(), 5.)\n",
    "              gen_optimizer.step()\n",
    "            else:\n",
    "              # since n_critic also defaults to 1, train every iteration\n",
    "              gen_z = torch.randn(pars[\"batch_size\"],pars[\"latent\"]).to(DEVICE)\n",
    "              gen_imgs = generator(gen_z)\n",
    "              fake_validity = discriminator(gen_imgs)\n",
    "\n",
    "              # calculate loss\n",
    "              loss_lz = torch.tensor(0)\n",
    "              real_label = torch.full((fake_validity.shape[0],fake_validity.shape[1]), 1., dtype=torch.float, device=DEVICE)\n",
    "              g_loss = nn.MSELoss()(fake_validity, real_label)\n",
    "              # skip accumulated_times since default is 1\n",
    "              g_loss.backward()\n",
    "\n",
    "              torch.nn.utils.clip_grad_norm_(generator.parameters(), 5.)\n",
    "              gen_optimizer.step()\n",
    "              gen_optimizer.zero_grad()\n",
    "\n",
    "            # schedulers\n",
    "            g_lr = gen_scheduler.step(global_steps)\n",
    "            d_lr = dis_scheduler.step(global_steps)\n",
    "\n",
    "            # moving average weight\n",
    "            ema_nimg = pars[\"ema_kimg\"] * 1000\n",
    "            cur_nimg = pars[\"batch_size\"] * global_steps\n",
    "\n",
    "            if pars[\"ema_warmup\"] != 0:\n",
    "                ema_nimg = min(ema_nimg, cur_nimg * pars[\"ema_warmup\"])\n",
    "                ema_beta = 0.5 ** (float(pars[\"batch_size\"]) / max(ema_nimg, 1e-8))\n",
    "            else:\n",
    "                ema_beta = pars[\"ema\"]\n",
    "            \n",
    "            # moving average weight\n",
    "            for p, avg_p in zip(generator.parameters(), gen_avg_param):\n",
    "                cpu_p = deepcopy(p)\n",
    "                avg_p.mul_(ema_beta).add_((1. - ema_beta + cpu_p.cpu().data).to(DEVICE))\n",
    "                del cpu_p\n",
    "\n",
    "        tqdm.write(\n",
    "                f\"[Epoch {epoch_idx + 1}/{n_epochs}] [Batch {(iter_idx % len(data_loader) + 1)}/{len(data_loader)}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] [ema: {ema_beta:.4f}] \")\n",
    "        torch.save(generator, path / f'./{model_name}_generator.pt')\n",
    "        torch.save(discriminator, path / f'./{model_name}_discriminator.pt')\n",
    "        if (epoch_idx and epoch_idx % 10 == 0) or epoch_idx == pars[\"n_epochs\"] - 1:\n",
    "          checkpoint[\"epoch\"] = epoch_idx\n",
    "          checkpoint['gen_net'] = generator\n",
    "          checkpoint['dis_net'] = discriminator\n",
    "          checkpoint['gen_scheduler'] = gen_scheduler\n",
    "          checkpoint['dis_scheduler'] = dis_scheduler\n",
    "          checkpoint['gen_optimizer'] = gen_optimizer\n",
    "          checkpoint['dis_optimizer'] = dis_optimizer\n",
    "          torch.save(checkpoint, path / f'{model_name}_checkpoint.pth',pickle_module=dill)\n",
    "          print(\"Checkpoint Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5z6XCTCi0Mx",
    "outputId": "2805abc7-7749-4107-e7c1-e60601864d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/11] [Batch 100/100] [D loss: -13.4343] [G loss: 4.7232] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/11] [Batch 100/100] [D loss: 0.8747] [G loss: -2.3898] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/11] [Batch 100/100] [D loss: 1.2891] [G loss: 0.1110] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/11] [Batch 100/100] [D loss: 1.0480] [G loss: 1.2922] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/11] [Batch 100/100] [D loss: 1.5584] [G loss: 0.5161] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/11] [Batch 100/100] [D loss: 1.6572] [G loss: 0.4290] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:13<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/11] [Batch 100/100] [D loss: 2.2454] [G loss: -0.7184] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/11] [Batch 100/100] [D loss: 2.5859] [G loss: -1.2685] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/11] [Batch 100/100] [D loss: 2.2155] [G loss: -1.4504] [ema: 0.0000] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/11] [Batch 100/100] [D loss: 1.9309] [G loss: -1.6242] [ema: 0.0000] \n",
      "Checkpoint Saved\n"
     ]
    }
   ],
   "source": [
    "train(data_loader,checkpoint=checkpoint,cond=cond)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
